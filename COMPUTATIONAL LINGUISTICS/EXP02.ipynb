{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lab 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import shakespeare\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('shakespeare')\n",
    "hamlet_text = shakespeare.raw('hamlet.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [word.translate(table) for word in tokens if word.isalpha()]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(tokens, n):\n",
    "    ngrams_list = []\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngram = tuple(tokens[i:i+n])\n",
    "        ngrams_list.append(ngram)\n",
    "    return ngrams_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_frequency(ngrams_list):\n",
    "    freq_dist = Counter(ngrams_list)\n",
    "    return freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = preprocess_text(hamlet_text)\n",
    "unigrams = generate_ngrams(tokens, 1)\n",
    "bigrams = generate_ngrams(tokens, 2)\n",
    "trigrams = generate_ngrams(tokens, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_freq = calculate_frequency(unigrams)\n",
    "bigrams_freq = calculate_frequency(bigrams)\n",
    "trigrams_freq = calculate_frequency(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bigram_probabilities(bigrams_list):\n",
    "    bigram_probs = {}\n",
    "    for bigram in bigrams_list:\n",
    "        prefix = bigram[0]\n",
    "        count_prefix = sum(1 for bg in bigrams_list if bg[0] == prefix)\n",
    "        bigram_probs[bigram] = bigrams_freq[bigram] / count_prefix\n",
    "    return bigram_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_next_word(word, bigram_probs):\n",
    "    candidates = [(bg[1], prob) for bg, prob in bigram_probs.items() if bg[0] == word]\n",
    "    if candidates:\n",
    "        next_word = max(candidates, key=lambda x: x[1])[0]\n",
    "    else:\n",
    "        next_word = None\n",
    "    return next_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package shakespeare to\n",
      "[nltk_data]     C:\\Users\\Shivam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\shakespeare.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams: [('xml',), ('doctype',), ('play',), ('system',), ('play',), ('title',), ('the',), ('tragedy',), ('of',), ('hamlet',)]\n",
      "Bigrams: [('xml', 'doctype'), ('doctype', 'play'), ('play', 'system'), ('system', 'play'), ('play', 'title'), ('title', 'the'), ('the', 'tragedy'), ('tragedy', 'of'), ('of', 'hamlet'), ('hamlet', 'prince')]\n",
      "Trigrams: [('xml', 'doctype', 'play'), ('doctype', 'play', 'system'), ('play', 'system', 'play'), ('system', 'play', 'title'), ('play', 'title', 'the'), ('title', 'the', 'tragedy'), ('the', 'tragedy', 'of'), ('tragedy', 'of', 'hamlet'), ('of', 'hamlet', 'prince'), ('hamlet', 'prince', 'of')]\n",
      "Unigrams frequency: [(('line',), 4016), (('speech',), 1153), (('speaker',), 1150), (('the',), 1145), (('and',), 970), (('to',), 743), (('of',), 673), (('i',), 632), (('a',), 537), (('you',), 531)]\n",
      "Bigrams frequency: [(('speech', 'speaker'), 1138), (('hamlet', 'line'), 377), (('speaker', 'hamlet'), 359), (('line', 'and'), 273), (('line', 'i'), 184), (('line', 'the'), 157), (('line', 'that'), 143), (('my', 'lord'), 125), (('king', 'claudius'), 120), (('horatio', 'line'), 118)]\n",
      "Trigrams frequency: [(('speech', 'speaker', 'hamlet'), 359), (('speaker', 'hamlet', 'line'), 359), (('speech', 'speaker', 'horatio'), 111), (('speaker', 'horatio', 'line'), 110), (('speech', 'speaker', 'king'), 102), (('speaker', 'king', 'claudius'), 102), (('king', 'claudius', 'line'), 102), (('speech', 'speaker', 'lord'), 89), (('speaker', 'lord', 'polonius'), 86), (('lord', 'polonius', 'line'), 86)]\n",
      "Probabilities for 'to be': 0.04441453566621804\n",
      "The next word after 'to' is 'the'\n"
     ]
    }
   ],
   "source": [
    "bigram_probs = calculate_bigram_probabilities(bigrams)\n",
    "word = 'to'\n",
    "next_word = find_next_word(word, bigram_probs)\n",
    "\n",
    "print(\"Unigrams:\", unigrams[:10])\n",
    "print(\"Bigrams:\", bigrams[:10])\n",
    "print(\"Trigrams:\", trigrams[:10])\n",
    "print(\"Unigrams frequency:\", unigrams_freq.most_common(10))\n",
    "print(\"Bigrams frequency:\", bigrams_freq.most_common(10))\n",
    "print(\"Trigrams frequency:\", trigrams_freq.most_common(10))\n",
    "print(\"Probabilities for 'to be':\", bigram_probs[('to', 'be')])\n",
    "print(\"The next word after '{0}' is '{1}'\".format(word, next_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package shakespeare to\n",
      "[nltk_data]     C:\\Users\\Shivam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package shakespeare is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Shivam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before pre-processing: ['<', '?', 'xml', 'version=', \"''\", '1.0', \"''\", '?', '>', '<']\n",
      "After pre-processing: ['xml', 'doctype', 'play', 'system', 'play', 'title', 'the', 'tragedy', 'of', 'hamlet']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import shakespeare\n",
    "nltk.download('shakespeare')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "hamlet_text = shakespeare.raw('hamlet.xml')\n",
    "tokens = word_tokenize(hamlet_text)\n",
    "\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "cleaned_tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "\n",
    "print(\"Before pre-processing:\", tokens[:10])\n",
    "print(\"After pre-processing:\", cleaned_tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Shivam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams: [('xml',), ('doctype',), ('play',), ('system',), ('play',), ('title',), ('the',), ('tragedy',), ('of',), ('hamlet',)]\n",
      "\n",
      " Bigrams: [('xml', 'doctype'), ('doctype', 'play'), ('play', 'system'), ('system', 'play'), ('play', 'title'), ('title', 'the'), ('the', 'tragedy'), ('tragedy', 'of'), ('of', 'hamlet'), ('hamlet', 'prince')]\n",
      "\n",
      " Trigrams: [('xml', 'doctype', 'play'), ('doctype', 'play', 'system'), ('play', 'system', 'play'), ('system', 'play', 'title'), ('play', 'title', 'the'), ('title', 'the', 'tragedy'), ('the', 'tragedy', 'of'), ('tragedy', 'of', 'hamlet'), ('of', 'hamlet', 'prince'), ('hamlet', 'prince', 'of')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [word.translate(table) for word in tokens if word.isalpha()]\n",
    "    return tokens\n",
    "\n",
    "def generate_ngrams(text, n):\n",
    "    tokens = preprocess_text(text)\n",
    "    ngrams_list = list(ngrams(tokens, n))\n",
    "    return ngrams_list\n",
    "\n",
    "hamlet_text = shakespeare.raw('hamlet.xml')\n",
    "unigrams = generate_ngrams(hamlet_text, 1)\n",
    "bigrams = generate_ngrams(hamlet_text, 2)\n",
    "trigrams = generate_ngrams(hamlet_text, 3)\n",
    "\n",
    "print(\"Unigrams:\", unigrams[:10])\n",
    "print(\"\\n Bigrams:\", bigrams[:10])\n",
    "print(\"\\n Trigrams:\", trigrams[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams frequency: [(('line',), 4016), (('speech',), 1153), (('speaker',), 1150), (('the',), 1145), (('and',), 970), (('to',), 743), (('of',), 673), (('i',), 632), (('a',), 537), (('you',), 531)]\n",
      "\n",
      " Bigrams frequency: [(('speech', 'speaker'), 1138), (('hamlet', 'line'), 377), (('speaker', 'hamlet'), 359), (('line', 'and'), 273), (('line', 'i'), 184), (('line', 'the'), 157), (('line', 'that'), 143), (('my', 'lord'), 125), (('king', 'claudius'), 120), (('horatio', 'line'), 118)]\n",
      "\n",
      " Trigrams frequency: [(('speech', 'speaker', 'hamlet'), 359), (('speaker', 'hamlet', 'line'), 359), (('speech', 'speaker', 'horatio'), 111), (('speaker', 'horatio', 'line'), 110), (('speech', 'speaker', 'king'), 102), (('speaker', 'king', 'claudius'), 102), (('king', 'claudius', 'line'), 102), (('speech', 'speaker', 'lord'), 89), (('speaker', 'lord', 'polonius'), 86), (('lord', 'polonius', 'line'), 86)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "def calculate_frequency(ngrams_list):\n",
    "    freq_dist = FreqDist(ngrams_list)\n",
    "    return freq_dist\n",
    "\n",
    "unigrams_freq = calculate_frequency(unigrams)\n",
    "bigrams_freq = calculate_frequency(bigrams)\n",
    "trigrams_freq = calculate_frequency(trigrams)\n",
    "\n",
    "print(\"Unigrams frequency:\", unigrams_freq.most_common(10))\n",
    "print(\"\\n Bigrams frequency:\", bigrams_freq.most_common(10))\n",
    "print(\"\\n Trigrams frequency:\", trigrams_freq.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities for 'the king': 0.0010394179259614615\n",
      "Probabilities for 'to be': 0.0008795074758135445\n"
     ]
    }
   ],
   "source": [
    "def calculate_bigram_probabilities(bigrams_list):\n",
    "    bigrams_freq = FreqDist(bigrams_list)\n",
    "    total_bigrams = len(bigrams_list)\n",
    "    bigram_probs = {bigram: bigrams_freq[bigram] / total_bigrams for bigram in bigrams_freq}\n",
    "    return bigram_probs\n",
    "\n",
    "bigram_probs = calculate_bigram_probabilities(bigrams)\n",
    "\n",
    "print(\"Probabilities for 'the king':\", bigram_probs[('the', 'king')])\n",
    "print(\"Probabilities for 'to be':\", bigram_probs[('to', 'be')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The next word after 'the' is 'line'\n"
     ]
    }
   ],
   "source": [
    "def find_next_word(word, bigram_probs):\n",
    "    next_word = max(bigram_probs, key=lambda x: (x[0] == word) * bigram_probs[x])\n",
    "    return next_word[1]\n",
    "\n",
    "word = 'the'\n",
    "next_word = find_next_word(word, bigram_probs)\n",
    "print(f\"The next word after '{word}' is '{next_word}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next word candidates after 'to': [('go', 0.1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Shivam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"I like to go for a walk in the park.\"\n",
    "\n",
    "tokens = word_tokenize(text.lower())\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "freq_dist = FreqDist(bigrams)\n",
    "\n",
    "\n",
    "total_bigrams = len(bigrams)\n",
    "bigram_probs = {bigram: freq_dist[bigram] / total_bigrams for bigram in freq_dist}\n",
    "\n",
    "word = 'to'\n",
    "next_word_candidates = [(bigram[1], prob) for bigram, prob in bigram_probs.items() if bigram[0] == word]\n",
    "next_word_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"Next word candidates after '{0}': {1}\".format(word, next_word_candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Shivam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'' , he said , `` and I hope that "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "\n",
    "nltk.download('brown')\n",
    "corpus = brown.words(categories='news')\n",
    "\n",
    "trigrams = list(ngrams(corpus, 3))\n",
    "\n",
    "trigram_freq = FreqDist(trigrams)\n",
    "cfd = ConditionalFreqDist(((w1, w2), w3) for w1, w2, w3 in trigrams)\n",
    "\n",
    "trigram_probs = {}\n",
    "for trigram in trigram_freq:\n",
    "    prefix = (trigram[0], trigram[1])\n",
    "    trigram_probs[trigram] = cfd[prefix][trigram[2]] / trigram_freq[trigram]\n",
    "\n",
    "prefix = ('the', 'economy')\n",
    "for _ in range(10):\n",
    "    next_word = max(cfd[prefix], key=cfd[prefix].get)\n",
    "    print(next_word, end=' ')\n",
    "    prefix = (prefix[1], next_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100554"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
